<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE | Welcome to my World</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[TOC] Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE  论文：pdf   B站李沐讲解视频   参考代码，Vision Transformer (ViT) 代码实现PyTorch版本 github 高星代码 有趣讲解代码 官网代码 huggingface代码 简易版，Visi">
<meta property="og:type" content="article">
<meta property="og:title" content="【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE">
<meta property="og:url" content="http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/index.html">
<meta property="og:site_name" content="Welcome to my World">
<meta property="og:description" content="[TOC] Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE  论文：pdf   B站李沐讲解视频   参考代码，Vision Transformer (ViT) 代码实现PyTorch版本 github 高星代码 有趣讲解代码 官网代码 huggingface代码 简易版，Visi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2023-10-23T07:23:33.000Z">
<meta property="article:modified_time" content="2023-10-24T11:22:11.132Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Classical Paper">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-24 19:22:11'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Welcome to my World"><span class="site-name">Welcome to my World</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-10-23T07:23:33.000Z" title="Created 2023-10-23 15:23:33">2023-10-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-10-24T11:22:11.132Z" title="Updated 2023-10-24 19:22:11">2023-10-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1>Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1>
<blockquote>
<p>论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">pdf</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.337.search-card.all.click&amp;vd_source=cd6df81a6a6c8f3e8be1f0e98f16f30b">B站李沐讲解视频</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/594622901">参考代码，Vision Transformer (ViT) 代码实现PyTorch版本</a><br>
<a target="_blank" rel="noopener" href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py#L83">github 高星代码</a><br>
<a target="_blank" rel="noopener" href="https://nn.labml.ai/transformers/vit/index.html">有趣讲解代码</a><br>
<a target="_blank" rel="noopener" href="https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py">官网代码</a><br>
<a target="_blank" rel="noopener" href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py">huggingface代码</a><br>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/412910412">简易版，Vision Transformer复现-原理及代码细节篇</a><br>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44966641/article/details/118733341">中文博客讲解</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_3d.py">vit_3d</a><br>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38736504/article/details/127594043">ViT 2D + 3D代码 实现,改写</a></p>
</blockquote>
<h2 id="Model">Model</h2>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/vit.png" class="" title="vit">
<h3 id="总体流程：">总体流程：</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/594622901">Vision Transformer (ViT) 代码实现PyTorch版本</a><br>
输入图片被划分为一个个16x16的小块，也叫做patch。接着这些patch被送入一个全连接层得到embeddings，然后在embeddings前再加上一个特殊的cls token。然后给所有的embedding加上位置信息编码positional encoding。接着这个整体被送入Transformer Encoder，然后取cls token的输出特征送入MLP Head去做分类。</p>
<h4 id="Patch-embeddings">Patch embeddings</h4>
<p>例如，输入图片大小<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>224</mn><mo>×</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224 \times 224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">224</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">224</span></span></span></span>, 即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>3</mn><mo separator="true">,</mo><mn>224</mn><mo separator="true">,</mo><mn>224</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1,3,224,224]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">224</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">224</span><span class="mclose">]</span></span></span></span>, patch_size=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16 \times 16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16</span></span></span></span>, 则一共可以划分成$\frac{(224 \times 224)}{(16 \times 16)} = 196 $ 个 patches，每个patch 大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>×</mo><mn>16</mn><mo>×</mo><mn>3</mn><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">16 \times 16 \times 3 = 768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">768</span></span></span></span> (沿通道展平)</p>
<p>然后将768 线性投影到 emb_size,也就是D维度，</p>
<p>⭐ 上述操作可以通往维度变换直接计算，也可通过卷积运算，卷积核大小和步长均为patch_size</p>
<p>1️⃣</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.projection = nn.Sequential(</span><br><span class="line">            <span class="comment"># 将原始图像切分为16*16的patch并把它们拉平</span></span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h s1) (w s2) -&gt; b (h w) (s1 s2 c)&#x27;</span>, s1=patch_size, s2=patch_size),</span><br><span class="line">            <span class="comment"># 注意这里的隐层大小设置的也是768，可以配置</span></span><br><span class="line">            nn.Linear(patch_size * patch_size * in_channels, emb_size))    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.projection(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># torch.Size([1, 196, 768])</span></span><br></pre></td></tr></table></figure>
<p>2️⃣</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.projection = nn.Sequential(</span><br><span class="line">            <span class="comment"># 使用一个卷积层而不是一个线性层 -&gt; 性能增加</span></span><br><span class="line">            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),</span><br><span class="line">            <span class="comment"># 将卷积操作后的patch铺平</span></span><br><span class="line">            Rearrange(<span class="string">&#x27;b e h w -&gt; b (h w) e&#x27;</span>),</span><br><span class="line">        )</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.projection(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># torch.Size([1, 196, 768])</span></span><br></pre></td></tr></table></figure>
<ul>
<li>什么是 Patch embeddings?
<blockquote>
<p>The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to D dimensions with a trainable linear projection.<br>
Transformer 在其所有层中使用恒定的潜在向量大小 D，因此我们将补丁展平并使用可训练的线性投影映射到 D 维度<br>
We refer to the output of this projection as the patch embeddings.<br>
我们将该投影的输出称为补丁嵌入。</p>
</blockquote>
</li>
</ul>
<h4 id="class-token">class token</h4>
<p>cls token是一个随机初始化的torch Parameter对象，在forward方法中它需要被拷贝b次(b是batch的数量), 然后使用torch.cat函数添加到patch前面。</p>
<p>在最后分类时，只使用class token部分的向量，不使用其他维度的向量。</p>
<p>The output of this token is then transformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity in the single hidden layer<br>
然后，该标记的输出通过小型多层感知器 (MLP) 转换为类预测，其中 tanh 在单个隐藏层中为非线性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.proj = nn.Sequential(</span><br><span class="line">            <span class="comment"># 使用一个卷积层而不是一个线性层 -&gt; 性能增加</span></span><br><span class="line">            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),</span><br><span class="line">            Rearrange(<span class="string">&#x27;b e (h) (w) -&gt; b (h w) e&#x27;</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 生成一个维度为emb_size的向量当做cls_token</span></span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, emb_size))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        b, _, _, _ = x.shape  <span class="comment"># 单独先将batch缓存起来</span></span><br><span class="line">        x = self.proj(x)  <span class="comment"># 进行卷积操作</span></span><br><span class="line">        <span class="comment"># 将cls_token 扩展b次</span></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;() n e -&gt; b n e&#x27;</span>, b=b)</span><br><span class="line">        <span class="built_in">print</span>(cls_tokens.shape) <span class="comment"># torch.Size([1, 1, 768])</span></span><br><span class="line">        <span class="built_in">print</span>(x.shape) <span class="comment"># torch.Size([1, 196, 768])</span></span><br><span class="line">        <span class="comment"># 将cls token在维度1扩展到输入上</span></span><br><span class="line">        x = torch.cat([cls_tokens, x], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x <span class="comment"># torch.Size([1, 197, 768])</span></span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape <span class="comment"># B, N+1, D</span></span><br></pre></td></tr></table></figure>
<h4 id="Position-Embeddings">Position Embeddings</h4>
<p>目前为止，模型还对patches在图像中的原始位置一无所知。我们需要传递给模型这些空间上的信息。可以有很多种方法来实现这个功能，在ViT中，我们让模型自己去学习这个。位置编码信息是一个形状为[(N_patches+1_token )* embed_size]的张量，它直接加到映射后的patches上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span>, img_size: <span class="built_in">int</span> = <span class="number">224</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.projection = nn.Sequential(</span><br><span class="line">            <span class="comment"># 使用一个卷积层而不是一个线性层 -&gt; 性能增加</span></span><br><span class="line">            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),</span><br><span class="line">            Rearrange(<span class="string">&#x27;b e (h) (w) -&gt; b (h w) e&#x27;</span>),</span><br><span class="line">        )</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>,<span class="number">1</span>, emb_size))</span><br><span class="line">        <span class="comment"># 位置编码信息，一共有(img_size // patch_size)**2 + 1(cls token)个位置向量</span></span><br><span class="line">        self.positions = nn.Parameter(torch.randn((img_size // patch_size)**<span class="number">2</span> + <span class="number">1</span>, emb_size))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        b, _, _, _ = x.shape</span><br><span class="line">        x = self.projection(x)</span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;() n e -&gt; b n e&#x27;</span>, b=b)</span><br><span class="line">        <span class="comment"># 将cls token在维度1扩展到输入上</span></span><br><span class="line">        x = torch.cat([cls_tokens, x], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 添加位置编码</span></span><br><span class="line">        <span class="built_in">print</span>(x.shape, self.positions.shape) <span class="comment"># torch.Size([1, 197, 768]) torch.Size([197, 768])</span></span><br><span class="line">        x += self.positions</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape <span class="comment"># torch.Size([1, 197, 768])</span></span><br></pre></td></tr></table></figure>
<p>其实位置嵌入pos_embed 的维度大小实际为[B, N+1, D],然后和嵌入了cls_token的patches 相加，大小还是[B, N+1, D]</p>
<p>⭐⭐⭐<br>
1️⃣ 关于position embedding的对比</p>
<ul>
<li>
<p>Providing no positional information(无位置信息)</p>
</li>
<li>
<p>1-dimensional positional embedding(1维线性位置信息)，raster order(光栅顺序，从左到右，从上到下)</p>
</li>
<li>
<p>2-dimensional positional embedding(2维位置信息，XY，每个占D/2个通道，再拼接成D通道)</p>
<blockquote>
<p>Considering the inputs as a grid of patches in two dimensions. In this case, two sets of embeddings are learned, each for one of the axes, X-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on the path in the input, we concatenate the X and Y embedding to get the final positional embedding for that patch.<br>
将输入视为二维补丁网格。在这种情况下，学习两组嵌入，每组嵌入一个轴、X 嵌入和 Y 嵌入，每个嵌入的大小为 D/2。然后，根据输入中路径上的坐标，我们连接 X 和 Y 嵌入以获得该补丁的最终位置嵌入。</p>
</blockquote>
</li>
<li>
<p>Relative positional embeddings(相对位置信息)</p>
<blockquote>
<p>Thus, for every given pair (one as query, and the other as key/value in the attention mechanism), we have an offset pq − pk, where each offset is associated with an embedding</p>
</blockquote>
</li>
</ul>
<p>2️⃣ 关于position embedding的结论</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/position.png" class="" title="Alt text">
<blockquote>
<p>“As we can see, while there is a large gap between the performances of the model with no positional embedding and models with positional embedding, there is little to no difference between different ways of encoding positional information.”<br>
正如我们所看到的，虽然没有位置嵌入的模型和有位置嵌入的模型的性能之间存在很大差距，但<strong>不同的位置信息编码方式之间几乎没有差异</strong>。</p>
</blockquote>
<blockquote>
<p>猜测解释：<br>
We speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important.<br>
我们推测，由于我们的 Transformer 编码器在块级输入上运行，而不是像素级输入，因此如何编码空间信息的差异并不那么重要。<br>
More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224,and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.<br>
更准确地说，在块级输入中，空间维度比原始像素级输入小得多，例如，14 × 14 而不是 224 × 224，对于这些不同的位置编码策略来说，学习表示该分辨率的空间关系同样容易。</p>
</blockquote>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/position2.png" class="" title="Alt text">
<blockquote>
<p>“Even so, the specific pattern of position embedding similarity learned by the network depends on the training hyperparameters” (<a href="zotero://select/library/items/8VPINMSG">Dosovitskiy 等, 2021, p. 18</a>) (<a href="zotero://open-pdf/library/items/JK5GUSXP?page=18&amp;annotation=P9FUVCS4">pdf</a>) 🔤即便如此，网络学习到的位置嵌入相似度的具体模式取决于训练超参数🔤</p>
</blockquote>
<h4 id="剩下的就是BN-MLP之类的模块，不难">剩下的就是BN,MLP之类的模块，不难</h4>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mi>C</mi></mrow></msup><mo>→</mo><msub><mi>x</mi><mi>p</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo>⋅</mo><mi>C</mi><mo stretchy="false">)</mo></mrow></msup><mo>→</mo><msub><mi>z</mi><mn>0</mn></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{H \times W \times C} 
\to x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}
\to z_0 \in \mathbb{R}^{(N+1) \times D}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8252em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9869em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span></span></span></span><br>
其中：<br>
$N = \frac{HW}{P^2} $, P是patch size</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/equation.png" class="" title="Alt text">
<h2 id="Experiments">Experiments</h2>
<p>详见论文</p>
<h2 id="如何理解ViT😳😳">如何理解ViT😳😳</h2>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig7.png" class="" title="Alt text">
<blockquote>
<p>图 7：左：ViT-L/32 RGB 值的初始线性嵌入的滤波器。中：ViT-L/32 位置嵌入的相似性。图块显示了具有指定行和列的补丁的位置嵌入与所有其他补丁的位置嵌入之间的余弦相似性。右图：按头部和网络深度划分的关注区域大小。每个点显示一层 16 个头部之一的图像之间的平均注意力距离。详细信息请参见附录 D.7。</p>
</blockquote>
<p>1️⃣ left: The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top principal components of the the learned embedding filters. The components resemble plausible basis functions for a low-dimensional representation of the fine structure within each patch.<br>
Vision Transformer 的第一层将展平的面片线性投影到低维空间（方程 1）。图 7（左）显示了学习到的嵌入过滤器的顶部主要组件。这些组件类似于每个补丁内精细结构的低维表示的合理基函数。</p>
<p>2️⃣center: Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position embeddings, i.e. closer patches tend to have more similar position embeddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology explains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4)图 7（中）显示，模型学习以位置嵌入的相似性对图像内的距离进行编码，即较近的块往往具有更相似的位置嵌入。进一步，出现行列结构；同一行/列中的补丁具有相似的嵌入。最后，对于较大的网格，有时会出现明显的正弦结构（附录 D）。位置嵌入学习表示 2D 图像拓扑解释了为什么手工制作的 2D 感知嵌入变体无法产生改进（附录 D.4）</p>
<hr>
<p><em><strong>attention distance</strong></em></p>
<p>3️⃣right: Self-attention allows ViT to integrate information across the entire image even in the lowest layers.自注意力使得 ViT 能够整合整个图像的信息，甚至在最低层也是如此。<br>
Specifically, we compute the average distance in image space across which information is integrated, based on the attention weights<br>
具体来说，我们根据注意力权重计算图像空间中信息整合的平均距离</p>
<p>⭐😮This “attention distance” is analogous to receptive field size in CNNs. We find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model.<strong>这种“注意力距离”类似于 CNN 中的感受野大小</strong>。我们发现一些头部已经关注了最低层中的大部分图像，这表明该模型确实使用了全局集成信息的能力。</p>
<p>⭐⭐</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig11.png" class="" title="Alt text">
<p>图 11：按头部和网络深度划分的关注区域大小。通过平均查询像素与所有其他像素之间的距离（按注意力权重加权），计算 128 个示例图像的注意力距离。每个点显示一层 16 个头部之一的图像之间的平均注意力距离。图像宽度为 224 像素。</p>
<ul>
<li>从图中能看出什么？</li>
<li>Average attention distance is highly variable across heads in lower layers, with some heads attending to much of the image, while others attend to small regions at or near the query location. As depth increases, attention distance increases for all heads.<br>
较低层中各个头部的平均注意力距离变化很大，一些头部关注图像的大部分，而其他头部则关注查询位置处或附近的小区域。随着深度的增加，所有头部的注意力距离都会增加。</li>
</ul>
<hr>
<p>Globally, we find that the model attends to image regions that are semantically relevant for classification<br>
我们发现该模型关注与分类语义相关的图像区域</p>
<h2 id="值得注意的细节😏😏😏">值得注意的细节😏😏😏</h2>
<ul>
<li>we do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step. Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP<br>
除了初始补丁提取步骤之外，我们不会将特定于图像的归纳偏差引入到架构中。相反，我们<code>将图像解释为一系列补丁</code>，并通过 NLP 中使用的标准 Transformer 编码器对其进行处理</li>
</ul>
<hr>
<ul>
<li>For ImageNet we found it beneficial to additionally apply gradient clipping at global norm 1<br>
对于 ImageNet，我们发现在全局范数 1 下额外应用梯度裁剪是有益的 <a href="#%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86">补充知识</a></li>
</ul>
<hr>
<ul>
<li>作者对网络结构进行了探索</li>
</ul>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig8.png" class="" title="Alt text">
<p>Decreasing the patch size and thus increasing the effective sequence length shows surprisingly robust improvements without introducing parameters.<br>
减小补丁大小并从而增加有效序列长度在不引入参数的情况下显示出令人惊讶的强大改进。</p>
<p>These findings suggest that compute might be a better predictor of performance than the number of parameters, and that scaling should emphasize depth over width if any.<br>
这些发现表明，计算可能比参数数量更好地预测性能，并且缩放应该强调深度而不是宽度（如果有的话）。</p>
<hr>
<h3 id="Axial-Attention😕">Axial Attention😕</h3>
<p>The general idea of axial attention is to perform multiple attention operations, each along a single axis of the input tensor, instead of applying 1-dimensional attention to the flattened version of the input.<br>
轴向注意力的总体思想是执行多个注意力操作，每个注意力操作都沿着输入张量的单个轴，而不是对输入的扁平版本应用一维注意力。<br>
In axial attention, each attention mixes information along a particular axis, while keeping information along the other axes independent.<br>
在轴向注意力中，每个注意力混合沿特定轴的信息，同时保持沿其他轴的信息独立。</p>
<ul>
<li>
<p>Axial Attention 效果怎么样？</p>
<blockquote>
<p>性能更好，计算更大</p>
</blockquote>
</li>
<li>
<p>为什么会这样？</p>
<blockquote>
<p>This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column selfattention and although the sequence length that self-attention operates on is smaller in axial case, there is a extra MLP per Axial-ViT block.<br>
这是因为在 Axial-ViT 模型中，每个具有全局自注意力的 Transformer 块被两个 Axial Transformer 块取代，一个具有行自注意力，一个具有列自注意力，尽管自注意力在轴向情况下运行的序列长度较小，每个 Axial-ViT 块都有一个额外的 MLP</p>
</blockquote>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig13.png" class="" title="Alt text">
</li>
</ul>
<h3 id="fine-tuning-微调">fine-tuning 微调</h3>
<p>“running fine-tuning at different resolution than training is common practice” ([Dosovitskiy 等, 2021, p. 14](Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.)) 🔤以与训练不同的分辨率进行微调是常见的做法🔤</p>
<p>ViT fine-tuning resolution: 384</p>
<ul>
<li>😕如何fine-tuning
<ul>
<li>pre-train on large datasets;</li>
<li>remove the pre-trained prediction head and attach a zero-initialized D × K feedforward layer, where K is the number of downstream classes.删除预训练的预测头并附加一个零初始化的 D × K 前馈层，其中 K 是下游类的数量。</li>
<li>fine-tune at higher resolution, keep the patch size the same</li>
<li>perform 2D interpolation of the pre-trained position embeddings</li>
</ul>
</li>
</ul>
<h3 id="self-supervision">self-supervision</h3>
<p>Self-Supervised Learning，又称为自监督学习，机器学习一般分为有监督学习，无监督学习和强化学习。 Self-Supervised Learning 是无监督学习里面的一种，希望学习一种通用的特征表达用于下游任务 (Downstream Tasks)。 主要通过自己监督自己。比如把一段话里面的几个单词去掉，用他的上下文去预测缺失的单词，或者将图片的一些部分去掉，依赖其周围的信息去预测缺失的 patch。</p>
<ul>
<li>
<p>为什么采用self-supervision?</p>
<blockquote>
<p>Transformers show impressive performance on NLP tasks. However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training<br>
Transformer 在 NLP 任务上表现出了令人印象深刻的性能。然而，他们的成功很大程度上不仅源于其出色的可扩展性，还源于大规模的自我监督预训练</p>
</blockquote>
</li>
<li>
<p>哪种方式？</p>
<blockquote>
<p>masked patch prediction</p>
</blockquote>
<blockquote>
<p>To do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable [mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is (10%).<br>
为此，我们通过将其嵌入替换为可学习的[掩码]嵌入（80％）、随机的其他补丁嵌入（10％）或仅保持原样（10％）来破坏50％的补丁嵌入。</p>
</blockquote>
</li>
<li>
<p>结论<br>
Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pretraining.<br>
我们的初步实验显示自监督预训练有所改善，但自监督预训练与大规模监督预训练之间仍然存在很大差距。</p>
</li>
</ul>
<h2 id="补充知识">补充知识</h2>
<h3 id="梯度截断">梯度截断</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/csnc007/article/details/97804398">梯度爆炸解决方案——梯度截断（gradient clip norm）</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/203085892">深度学习之梯度裁剪（Gradient Clipping）</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/379032332">Gradient Clipping理解</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353871841">梯度裁剪clip_grad_norm和clip_gradient</a></p>
<p>梯度剪裁是一种在网络反向传播过程中，将误差导数改变或剪裁到阈值，并利用剪裁后的梯度来更新权值的方法。</p>
<p>我们可以采取一个简单的策略来避免梯度的爆炸，那就是梯度截断Clip, 将梯度约束在某一个区间之内，在训练的过程中，在优化器更新之前进行梯度截断操作。</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/gradient_clipping.png" class="" title="梯度裁剪">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn.utils <span class="keyword">import</span> clip_grad_norm</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()        </span><br><span class="line">loss, hidden = model(data, hidden, targets)</span><br><span class="line">loss.backward()</span><br><span class="line"> </span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个函数计算的是全局梯度范数</span></span><br><span class="line"><span class="comment"># torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=5, norm_type=2)</span></span><br><span class="line"><span class="comment"># parameters: an iterable of Variables that will have gradients normalized</span></span><br><span class="line"><span class="comment"># max_norm: max norm of the gradients(阈值设定)</span></span><br><span class="line"><span class="comment"># norm_type: type of the used p-norm. Can be&#x27;inf&#x27;for infinity norm(定义范数类型)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353871841">https://zhuanlan.zhihu.com/p/353871841</a></p>
<h3 id="few-shot-learning-少样本学习">few-shot learning(少样本学习)</h3>
<p>Few-shot Learning 是 Meta Learning 在监督学习领域的应用。<br>
Meta Learning，又称为learning to learn，该算法旨在让模型学会“学习”，能够处理类型相似的任务，而不是只会单一的分类任务。</p>
<p>few-shot learning 中有一个术语叫做 N-way K -shot 问题。<br>
形式化来说，few-shot 的训练集中包含了很多的类别，每个类别中有多个样本。在训练阶段，会在训练集中随机抽取 N 个类别，每个类别 K 个样本（总共 N<em>K 个数据），构建一个 meta-task，作为模型的支撑集（support set）输入；再从这 N 个类中剩余的数据中抽取一批（batch）样本作为模型的预测对象（batch set）。即要求模型从 N</em>K 个数据中学会如何区分这 N 个类别，这样的任务被称为 N-way K-shot 问题。</p>
<p>图2展示的是一个 2-way 5-shot 的示例，可以看到 meta training 阶段构建了一系列 meta-task 来让模型学习如何根据 support set 预测 batch set 中的样本的标签；meta testing 阶段的输入数据的形式与训练阶段一致（2-way 5-shot），但是会在全新的类别上构建 support set 和 batch。</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/few-shot.png" class="" title="Alt text">
<h2 id="好词佳句积累">好词佳句积累</h2>
<ul>
<li>
<p>While large ViT models <strong>perform worse</strong> than BiT ResNets (shaded area) when pre-trained on small datasets, they <strong>shine</strong> when pre-trained on larger datasets. Similarly, larger ViT variants <strong>overtake</strong> smaller ones as the dataset grows.</p>
</li>
<li>
<p>ResNets perform better with smaller pre-training datasets <strong>but plateau sooner than</strong> ViT, which performs better with larger pre-training.</p>
</li>
</ul>
<h2 id="附录">附录</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py#L83">GitHub 15k⭐代码</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br><span class="line"><span class="comment"># helpers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pair</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">else</span> (t, t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># classes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line"></span><br><span class="line">        self.attend = nn.Softmax(dim = -<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.norm(x)</span><br><span class="line"></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)</span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = self.heads), qkv)</span><br><span class="line"></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale</span><br><span class="line"></span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        out = torch.matmul(attn, v)</span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.layers = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),</span><br><span class="line">                FeedForward(dim, mlp_dim, dropout = dropout)</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span>, emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_height, image_width = pair(image_size)</span><br><span class="line">        patch_height, patch_width = pair(patch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width // patch_width)</span><br><span class="line">        patch_dim = channels * patch_height * patch_width</span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_height, p2 = patch_width),</span><br><span class="line">            nn.LayerNorm(patch_dim),</span><br><span class="line">            nn.Linear(patch_dim, dim),</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)</span><br><span class="line"></span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.mlp_head = nn.Linear(dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)</span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;1 1 d -&gt; b 1 d&#x27;</span>, b = b)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line"></span><br><span class="line">        x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/">http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Classical-Paper/">Classical Paper</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/26/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2023_Video_Frame_Interpolation_Survey/" title="【Video】Video Frame Interpolation A Comprehensive Survey"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">【Video】Video Frame Interpolation A Comprehensive Survey</div></div></a></div><div class="next-post pull-right"><a href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-12</div><div class="title">【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">John Doe</div><div class="author-info__description">Galaxy</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">1.1.</span> <span class="toc-text">Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E4%BD%93%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-number">1.1.1.</span> <span class="toc-text">总体流程：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Patch-embeddings"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Patch embeddings</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#class-token"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">class token</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Position-Embeddings"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">Position Embeddings</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%A9%E4%B8%8B%E7%9A%84%E5%B0%B1%E6%98%AFBN-MLP%E4%B9%8B%E7%B1%BB%E7%9A%84%E6%A8%A1%E5%9D%97%EF%BC%8C%E4%B8%8D%E9%9A%BE"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">剩下的就是BN,MLP之类的模块，不难</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiments"><span class="toc-number">1.2.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3ViT%F0%9F%98%B3%F0%9F%98%B3"><span class="toc-number">1.3.</span> <span class="toc-text">如何理解ViT😳😳</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%BE%97%E6%B3%A8%E6%84%8F%E7%9A%84%E7%BB%86%E8%8A%82%F0%9F%98%8F%F0%9F%98%8F%F0%9F%98%8F"><span class="toc-number">1.4.</span> <span class="toc-text">值得注意的细节😏😏😏</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Axial-Attention%F0%9F%98%95"><span class="toc-number">1.4.1.</span> <span class="toc-text">Axial Attention😕</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fine-tuning-%E5%BE%AE%E8%B0%83"><span class="toc-number">1.4.2.</span> <span class="toc-text">fine-tuning 微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-supervision"><span class="toc-number">1.4.3.</span> <span class="toc-text">self-supervision</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86"><span class="toc-number">1.5.</span> <span class="toc-text">补充知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%88%AA%E6%96%AD"><span class="toc-number">1.5.1.</span> <span class="toc-text">梯度截断</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#few-shot-learning-%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.5.2.</span> <span class="toc-text">few-shot learning(少样本学习)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A5%BD%E8%AF%8D%E4%BD%B3%E5%8F%A5%E7%A7%AF%E7%B4%AF"><span class="toc-number">1.6.</span> <span class="toc-text">好词佳句积累</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">1.7.</span> <span class="toc-text">附录</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/27/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2018_SuperSloMo/" title="【Video】Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Video】Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation"/></a><div class="content"><a class="title" href="/2023/10/27/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2018_SuperSloMo/" title="【Video】Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation">【Video】Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation</a><time datetime="2023-10-27T03:16:59.000Z" title="Created 2023-10-27 11:16:59">2023-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/26/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2023_Video_Frame_Interpolation_Survey/" title="【Video】Video Frame Interpolation A Comprehensive Survey"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Video】Video Frame Interpolation A Comprehensive Survey"/></a><div class="content"><a class="title" href="/2023/10/26/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2023_Video_Frame_Interpolation_Survey/" title="【Video】Video Frame Interpolation A Comprehensive Survey">【Video】Video Frame Interpolation A Comprehensive Survey</a><time datetime="2023-10-26T03:28:12.000Z" title="Created 2023-10-26 11:28:12">2023-10-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/" title="【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"/></a><div class="content"><a class="title" href="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/" title="【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE">【Classical Paper】AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a><time datetime="2023-10-23T07:23:33.000Z" title="Created 2023-10-23 15:23:33">2023-10-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks"/></a><div class="content"><a class="title" href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks">【Classical Paper】ImageNet Classification with Deep Convolutional Neural Networks</a><time datetime="2023-10-12T03:29:47.000Z" title="Created 2023-10-12 11:29:47">2023-10-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/10/First-post/" title="First post"><img src="/img/cat.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="First post"/></a><div class="content"><a class="title" href="/2023/10/10/First-post/" title="First post">First post</a><time datetime="2023-10-10T05:11:00.000Z" title="Created 2023-10-10 13:11:00">2023-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By John Doe</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":320,"height":480},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body></html>