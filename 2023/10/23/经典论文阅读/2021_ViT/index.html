<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE | Welcome to my World</title><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[TOC] Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE  è®ºæ–‡ï¼špdf   Bç«™ææ²è®²è§£è§†é¢‘   å‚è€ƒä»£ç ï¼ŒVision Transformer (ViT) ä»£ç å®ç°PyTorchç‰ˆæœ¬ github é«˜æ˜Ÿä»£ç  æœ‰è¶£è®²è§£ä»£ç  å®˜ç½‘ä»£ç  huggingfaceä»£ç  ç®€æ˜“ç‰ˆï¼ŒVisi">
<meta property="og:type" content="article">
<meta property="og:title" content="ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE">
<meta property="og:url" content="http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/index.html">
<meta property="og:site_name" content="Welcome to my World">
<meta property="og:description" content="[TOC] Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE  è®ºæ–‡ï¼špdf   Bç«™ææ²è®²è§£è§†é¢‘   å‚è€ƒä»£ç ï¼ŒVision Transformer (ViT) ä»£ç å®ç°PyTorchç‰ˆæœ¬ github é«˜æ˜Ÿä»£ç  æœ‰è¶£è®²è§£ä»£ç  å®˜ç½‘ä»£ç  huggingfaceä»£ç  ç®€æ˜“ç‰ˆï¼ŒVisi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2023-10-23T07:23:33.000Z">
<meta property="article:modified_time" content="2023-10-24T11:22:11.132Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Classical Paper">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-24 19:22:11'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Welcome to my World"><span class="site-name">Welcome to my World</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-10-23T07:23:33.000Z" title="Created 2023-10-23 15:23:33">2023-10-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-10-24T11:22:11.132Z" title="Updated 2023-10-24 19:22:11">2023-10-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1>Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1>
<blockquote>
<p>è®ºæ–‡ï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">pdf</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.337.search-card.all.click&amp;vd_source=cd6df81a6a6c8f3e8be1f0e98f16f30b">Bç«™ææ²è®²è§£è§†é¢‘</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/594622901">å‚è€ƒä»£ç ï¼ŒVision Transformer (ViT) ä»£ç å®ç°PyTorchç‰ˆæœ¬</a><br>
<a target="_blank" rel="noopener" href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py#L83">github é«˜æ˜Ÿä»£ç </a><br>
<a target="_blank" rel="noopener" href="https://nn.labml.ai/transformers/vit/index.html">æœ‰è¶£è®²è§£ä»£ç </a><br>
<a target="_blank" rel="noopener" href="https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py">å®˜ç½‘ä»£ç </a><br>
<a target="_blank" rel="noopener" href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py">huggingfaceä»£ç </a><br>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/412910412">ç®€æ˜“ç‰ˆï¼ŒVision Transformerå¤ç°-åŸç†åŠä»£ç ç»†èŠ‚ç¯‡</a><br>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44966641/article/details/118733341">ä¸­æ–‡åšå®¢è®²è§£</a></p>
</blockquote>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit_3d.py">vit_3d</a><br>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38736504/article/details/127594043">ViT 2D + 3Dä»£ç  å®ç°,æ”¹å†™</a></p>
</blockquote>
<h2 id="Model">Model</h2>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/vit.png" class="" title="vit">
<h3 id="æ€»ä½“æµç¨‹ï¼š">æ€»ä½“æµç¨‹ï¼š</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/594622901">Vision Transformer (ViT) ä»£ç å®ç°PyTorchç‰ˆæœ¬</a><br>
è¾“å…¥å›¾ç‰‡è¢«åˆ’åˆ†ä¸ºä¸€ä¸ªä¸ª16x16çš„å°å—ï¼Œä¹Ÿå«åšpatchã€‚æ¥ç€è¿™äº›patchè¢«é€å…¥ä¸€ä¸ªå…¨è¿æ¥å±‚å¾—åˆ°embeddingsï¼Œç„¶ååœ¨embeddingså‰å†åŠ ä¸Šä¸€ä¸ªç‰¹æ®Šçš„cls tokenã€‚ç„¶åç»™æ‰€æœ‰çš„embeddingåŠ ä¸Šä½ç½®ä¿¡æ¯ç¼–ç positional encodingã€‚æ¥ç€è¿™ä¸ªæ•´ä½“è¢«é€å…¥Transformer Encoderï¼Œç„¶åå–cls tokençš„è¾“å‡ºç‰¹å¾é€å…¥MLP Headå»åšåˆ†ç±»ã€‚</p>
<h4 id="Patch-embeddings">Patch embeddings</h4>
<p>ä¾‹å¦‚ï¼Œè¾“å…¥å›¾ç‰‡å¤§å°<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>224</mn><mo>Ã—</mo><mn>224</mn></mrow><annotation encoding="application/x-tex">224 \times 224</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">224</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">224</span></span></span></span>, å³<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>3</mn><mo separator="true">,</mo><mn>224</mn><mo separator="true">,</mo><mn>224</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1,3,224,224]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">224</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">224</span><span class="mclose">]</span></span></span></span>, patch_size=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>Ã—</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">16 \times 16</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16</span></span></span></span>, åˆ™ä¸€å…±å¯ä»¥åˆ’åˆ†æˆ$\frac{(224 \times 224)}{(16 \times 16)} = 196 $ ä¸ª patchesï¼Œæ¯ä¸ªpatch å¤§å°ä¸º<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>16</mn><mo>Ã—</mo><mn>16</mn><mo>Ã—</mo><mn>3</mn><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">16 \times 16 \times 3 = 768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">768</span></span></span></span> (æ²¿é€šé“å±•å¹³)</p>
<p>ç„¶åå°†768 çº¿æ€§æŠ•å½±åˆ° emb_size,ä¹Ÿå°±æ˜¯Dç»´åº¦ï¼Œ</p>
<p>â­ ä¸Šè¿°æ“ä½œå¯ä»¥é€šå¾€ç»´åº¦å˜æ¢ç›´æ¥è®¡ç®—ï¼Œä¹Ÿå¯é€šè¿‡å·ç§¯è¿ç®—ï¼Œå·ç§¯æ ¸å¤§å°å’Œæ­¥é•¿å‡ä¸ºpatch_size</p>
<p>1ï¸âƒ£</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.projection = nn.Sequential(</span><br><span class="line">            <span class="comment"># å°†åŸå§‹å›¾åƒåˆ‡åˆ†ä¸º16*16çš„patchå¹¶æŠŠå®ƒä»¬æ‹‰å¹³</span></span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h s1) (w s2) -&gt; b (h w) (s1 s2 c)&#x27;</span>, s1=patch_size, s2=patch_size),</span><br><span class="line">            <span class="comment"># æ³¨æ„è¿™é‡Œçš„éšå±‚å¤§å°è®¾ç½®çš„ä¹Ÿæ˜¯768ï¼Œå¯ä»¥é…ç½®</span></span><br><span class="line">            nn.Linear(patch_size * patch_size * in_channels, emb_size))    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.projection(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¾“å‡º</span></span><br><span class="line"><span class="comment"># torch.Size([1, 196, 768])</span></span><br></pre></td></tr></table></figure>
<p>2ï¸âƒ£</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.projection = nn.Sequential(</span><br><span class="line">            <span class="comment"># ä½¿ç”¨ä¸€ä¸ªå·ç§¯å±‚è€Œä¸æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ -&gt; æ€§èƒ½å¢åŠ </span></span><br><span class="line">            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),</span><br><span class="line">            <span class="comment"># å°†å·ç§¯æ“ä½œåçš„patché“ºå¹³</span></span><br><span class="line">            Rearrange(<span class="string">&#x27;b e h w -&gt; b (h w) e&#x27;</span>),</span><br><span class="line">        )</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        x = self.projection(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¾“å‡º</span></span><br><span class="line"><span class="comment"># torch.Size([1, 196, 768])</span></span><br></pre></td></tr></table></figure>
<ul>
<li>ä»€ä¹ˆæ˜¯ Patch embeddings?
<blockquote>
<p>The Transformer uses constant latent vector size D through all of its layers, so we flatten the patches and map to D dimensions with a trainable linear projection.<br>
Transformer åœ¨å…¶æ‰€æœ‰å±‚ä¸­ä½¿ç”¨æ’å®šçš„æ½œåœ¨å‘é‡å¤§å° Dï¼Œå› æ­¤æˆ‘ä»¬å°†è¡¥ä¸å±•å¹³å¹¶ä½¿ç”¨å¯è®­ç»ƒçš„çº¿æ€§æŠ•å½±æ˜ å°„åˆ° D ç»´åº¦<br>
We refer to the output of this projection as the patch embeddings.<br>
æˆ‘ä»¬å°†è¯¥æŠ•å½±çš„è¾“å‡ºç§°ä¸ºè¡¥ä¸åµŒå…¥ã€‚</p>
</blockquote>
</li>
</ul>
<h4 id="class-token">class token</h4>
<p>cls tokenæ˜¯ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„torch Parameterå¯¹è±¡ï¼Œåœ¨forwardæ–¹æ³•ä¸­å®ƒéœ€è¦è¢«æ‹·è´bæ¬¡(bæ˜¯batchçš„æ•°é‡), ç„¶åä½¿ç”¨torch.catå‡½æ•°æ·»åŠ åˆ°patchå‰é¢ã€‚</p>
<p>åœ¨æœ€ååˆ†ç±»æ—¶ï¼Œåªä½¿ç”¨class tokenéƒ¨åˆ†çš„å‘é‡ï¼Œä¸ä½¿ç”¨å…¶ä»–ç»´åº¦çš„å‘é‡ã€‚</p>
<p>The output of this token is then transformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity in the single hidden layer<br>
ç„¶åï¼Œè¯¥æ ‡è®°çš„è¾“å‡ºé€šè¿‡å°å‹å¤šå±‚æ„ŸçŸ¥å™¨ (MLP) è½¬æ¢ä¸ºç±»é¢„æµ‹ï¼Œå…¶ä¸­ tanh åœ¨å•ä¸ªéšè—å±‚ä¸­ä¸ºéçº¿æ€§</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.proj = nn.Sequential(</span><br><span class="line">            <span class="comment"># ä½¿ç”¨ä¸€ä¸ªå·ç§¯å±‚è€Œä¸æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ -&gt; æ€§èƒ½å¢åŠ </span></span><br><span class="line">            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),</span><br><span class="line">            Rearrange(<span class="string">&#x27;b e (h) (w) -&gt; b (h w) e&#x27;</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># ç”Ÿæˆä¸€ä¸ªç»´åº¦ä¸ºemb_sizeçš„å‘é‡å½“åšcls_token</span></span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, emb_size))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        b, _, _, _ = x.shape  <span class="comment"># å•ç‹¬å…ˆå°†batchç¼“å­˜èµ·æ¥</span></span><br><span class="line">        x = self.proj(x)  <span class="comment"># è¿›è¡Œå·ç§¯æ“ä½œ</span></span><br><span class="line">        <span class="comment"># å°†cls_token æ‰©å±•bæ¬¡</span></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;() n e -&gt; b n e&#x27;</span>, b=b)</span><br><span class="line">        <span class="built_in">print</span>(cls_tokens.shape) <span class="comment"># torch.Size([1, 1, 768])</span></span><br><span class="line">        <span class="built_in">print</span>(x.shape) <span class="comment"># torch.Size([1, 196, 768])</span></span><br><span class="line">        <span class="comment"># å°†cls tokenåœ¨ç»´åº¦1æ‰©å±•åˆ°è¾“å…¥ä¸Š</span></span><br><span class="line">        x = torch.cat([cls_tokens, x], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x <span class="comment"># torch.Size([1, 197, 768])</span></span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape <span class="comment"># B, N+1, D</span></span><br></pre></td></tr></table></figure>
<h4 id="Position-Embeddings">Position Embeddings</h4>
<p>ç›®å‰ä¸ºæ­¢ï¼Œæ¨¡å‹è¿˜å¯¹patchesåœ¨å›¾åƒä¸­çš„åŸå§‹ä½ç½®ä¸€æ— æ‰€çŸ¥ã€‚æˆ‘ä»¬éœ€è¦ä¼ é€’ç»™æ¨¡å‹è¿™äº›ç©ºé—´ä¸Šçš„ä¿¡æ¯ã€‚å¯ä»¥æœ‰å¾ˆå¤šç§æ–¹æ³•æ¥å®ç°è¿™ä¸ªåŠŸèƒ½ï¼Œåœ¨ViTä¸­ï¼Œæˆ‘ä»¬è®©æ¨¡å‹è‡ªå·±å»å­¦ä¹ è¿™ä¸ªã€‚ä½ç½®ç¼–ç ä¿¡æ¯æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º[(N_patches+1_token )* embed_size]çš„å¼ é‡ï¼Œå®ƒç›´æ¥åŠ åˆ°æ˜ å°„åçš„patchesä¸Šã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span> = <span class="number">3</span>, patch_size: <span class="built_in">int</span> = <span class="number">16</span>, emb_size: <span class="built_in">int</span> = <span class="number">768</span>, img_size: <span class="built_in">int</span> = <span class="number">224</span></span>):</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.projection = nn.Sequential(</span><br><span class="line">            <span class="comment"># ä½¿ç”¨ä¸€ä¸ªå·ç§¯å±‚è€Œä¸æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ -&gt; æ€§èƒ½å¢åŠ </span></span><br><span class="line">            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),</span><br><span class="line">            Rearrange(<span class="string">&#x27;b e (h) (w) -&gt; b (h w) e&#x27;</span>),</span><br><span class="line">        )</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>,<span class="number">1</span>, emb_size))</span><br><span class="line">        <span class="comment"># ä½ç½®ç¼–ç ä¿¡æ¯ï¼Œä¸€å…±æœ‰(img_size // patch_size)**2 + 1(cls token)ä¸ªä½ç½®å‘é‡</span></span><br><span class="line">        self.positions = nn.Parameter(torch.randn((img_size // patch_size)**<span class="number">2</span> + <span class="number">1</span>, emb_size))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: Tensor</span>) -&gt; Tensor:</span><br><span class="line">        b, _, _, _ = x.shape</span><br><span class="line">        x = self.projection(x)</span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;() n e -&gt; b n e&#x27;</span>, b=b)</span><br><span class="line">        <span class="comment"># å°†cls tokenåœ¨ç»´åº¦1æ‰©å±•åˆ°è¾“å…¥ä¸Š</span></span><br><span class="line">        x = torch.cat([cls_tokens, x], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># æ·»åŠ ä½ç½®ç¼–ç </span></span><br><span class="line">        <span class="built_in">print</span>(x.shape, self.positions.shape) <span class="comment"># torch.Size([1, 197, 768]) torch.Size([197, 768])</span></span><br><span class="line">        x += self.positions</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">PatchEmbedding()(x).shape <span class="comment"># torch.Size([1, 197, 768])</span></span><br></pre></td></tr></table></figure>
<p>å…¶å®ä½ç½®åµŒå…¥pos_embed çš„ç»´åº¦å¤§å°å®é™…ä¸º[B, N+1, D],ç„¶åå’ŒåµŒå…¥äº†cls_tokençš„patches ç›¸åŠ ï¼Œå¤§å°è¿˜æ˜¯[B, N+1, D]</p>
<p>â­â­â­<br>
1ï¸âƒ£ å…³äºposition embeddingçš„å¯¹æ¯”</p>
<ul>
<li>
<p>Providing no positional information(æ— ä½ç½®ä¿¡æ¯)</p>
</li>
<li>
<p>1-dimensional positional embedding(1ç»´çº¿æ€§ä½ç½®ä¿¡æ¯)ï¼Œraster order(å…‰æ …é¡ºåºï¼Œä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹)</p>
</li>
<li>
<p>2-dimensional positional embedding(2ç»´ä½ç½®ä¿¡æ¯ï¼ŒXYï¼Œæ¯ä¸ªå D/2ä¸ªé€šé“ï¼Œå†æ‹¼æ¥æˆDé€šé“)</p>
<blockquote>
<p>Considering the inputs as a grid of patches in two dimensions. In this case, two sets of embeddings are learned, each for one of the axes, X-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on the path in the input, we concatenate the X and Y embedding to get the final positional embedding for that patch.<br>
å°†è¾“å…¥è§†ä¸ºäºŒç»´è¡¥ä¸ç½‘æ ¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå­¦ä¹ ä¸¤ç»„åµŒå…¥ï¼Œæ¯ç»„åµŒå…¥ä¸€ä¸ªè½´ã€X åµŒå…¥å’Œ Y åµŒå…¥ï¼Œæ¯ä¸ªåµŒå…¥çš„å¤§å°ä¸º D/2ã€‚ç„¶åï¼Œæ ¹æ®è¾“å…¥ä¸­è·¯å¾„ä¸Šçš„åæ ‡ï¼Œæˆ‘ä»¬è¿æ¥ X å’Œ Y åµŒå…¥ä»¥è·å¾—è¯¥è¡¥ä¸çš„æœ€ç»ˆä½ç½®åµŒå…¥ã€‚</p>
</blockquote>
</li>
<li>
<p>Relative positional embeddings(ç›¸å¯¹ä½ç½®ä¿¡æ¯)</p>
<blockquote>
<p>Thus, for every given pair (one as query, and the other as key/value in the attention mechanism), we have an offset pq âˆ’ pk, where each offset is associated with an embedding</p>
</blockquote>
</li>
</ul>
<p>2ï¸âƒ£ å…³äºposition embeddingçš„ç»“è®º</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/position.png" class="" title="Alt text">
<blockquote>
<p>â€œAs we can see, while there is a large gap between the performances of the model with no positional embedding and models with positional embedding, there is little to no difference between different ways of encoding positional information.â€<br>
æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè™½ç„¶æ²¡æœ‰ä½ç½®åµŒå…¥çš„æ¨¡å‹å’Œæœ‰ä½ç½®åµŒå…¥çš„æ¨¡å‹çš„æ€§èƒ½ä¹‹é—´å­˜åœ¨å¾ˆå¤§å·®è·ï¼Œä½†<strong>ä¸åŒçš„ä½ç½®ä¿¡æ¯ç¼–ç æ–¹å¼ä¹‹é—´å‡ ä¹æ²¡æœ‰å·®å¼‚</strong>ã€‚</p>
</blockquote>
<blockquote>
<p>çŒœæµ‹è§£é‡Šï¼š<br>
We speculate that since our Transformer encoder operates on patch-level inputs, as opposed to pixel-level, the differences in how to encode spatial information is less important.<br>
æˆ‘ä»¬æ¨æµ‹ï¼Œç”±äºæˆ‘ä»¬çš„ Transformer ç¼–ç å™¨åœ¨å—çº§è¾“å…¥ä¸Šè¿è¡Œï¼Œè€Œä¸æ˜¯åƒç´ çº§è¾“å…¥ï¼Œå› æ­¤å¦‚ä½•ç¼–ç ç©ºé—´ä¿¡æ¯çš„å·®å¼‚å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚<br>
More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original pixel-level inputs, e.g., 14 Ã— 14 as opposed to 224 Ã— 224,and learning to represent the spatial relations in this resolution is equally easy for these different positional encoding strategies.<br>
æ›´å‡†ç¡®åœ°è¯´ï¼Œåœ¨å—çº§è¾“å…¥ä¸­ï¼Œç©ºé—´ç»´åº¦æ¯”åŸå§‹åƒç´ çº§è¾“å…¥å°å¾—å¤šï¼Œä¾‹å¦‚ï¼Œ14 Ã— 14 è€Œä¸æ˜¯ 224 Ã— 224ï¼Œå¯¹äºè¿™äº›ä¸åŒçš„ä½ç½®ç¼–ç ç­–ç•¥æ¥è¯´ï¼Œå­¦ä¹ è¡¨ç¤ºè¯¥åˆ†è¾¨ç‡çš„ç©ºé—´å…³ç³»åŒæ ·å®¹æ˜“ã€‚</p>
</blockquote>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/position2.png" class="" title="Alt text">
<blockquote>
<p>â€œEven so, the specific pattern of position embedding similarity learned by the network depends on the training hyperparametersâ€ (<a href="zotero://select/library/items/8VPINMSG">Dosovitskiy ç­‰, 2021, p. 18</a>) (<a href="zotero://open-pdf/library/items/JK5GUSXP?page=18&amp;annotation=P9FUVCS4">pdf</a>) ğŸ”¤å³ä¾¿å¦‚æ­¤ï¼Œç½‘ç»œå­¦ä¹ åˆ°çš„ä½ç½®åµŒå…¥ç›¸ä¼¼åº¦çš„å…·ä½“æ¨¡å¼å–å†³äºè®­ç»ƒè¶…å‚æ•°ğŸ”¤</p>
</blockquote>
<h4 id="å‰©ä¸‹çš„å°±æ˜¯BN-MLPä¹‹ç±»çš„æ¨¡å—ï¼Œä¸éš¾">å‰©ä¸‹çš„å°±æ˜¯BN,MLPä¹‹ç±»çš„æ¨¡å—ï¼Œä¸éš¾</h4>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>âˆˆ</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>H</mi><mo>Ã—</mo><mi>W</mi><mo>Ã—</mo><mi>C</mi></mrow></msup><mo>â†’</mo><msub><mi>x</mi><mi>p</mi></msub><mo>âˆˆ</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>Ã—</mo><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo>â‹…</mo><mi>C</mi><mo stretchy="false">)</mo></mrow></msup><mo>â†’</mo><msub><mi>z</mi><mn>0</mn></msub><mo>âˆˆ</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>Ã—</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^{H \times W \times C} 
\to x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}
\to z_0 \in \mathbb{R}^{(N+1) \times D}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">âˆˆ</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span><span class="mbin mtight">Ã—</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">W</span><span class="mbin mtight">Ã—</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">â†’</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8252em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">âˆˆ</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9869em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9869em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">Ã—</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">â‹…</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">â†’</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">âˆˆ</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span><span class="mbin mtight">Ã—</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span></span></span></span></span></span></span></span><br>
å…¶ä¸­ï¼š<br>
$N = \frac{HW}{P^2} $, Pæ˜¯patch size</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/equation.png" class="" title="Alt text">
<h2 id="Experiments">Experiments</h2>
<p>è¯¦è§è®ºæ–‡</p>
<h2 id="å¦‚ä½•ç†è§£ViTğŸ˜³ğŸ˜³">å¦‚ä½•ç†è§£ViTğŸ˜³ğŸ˜³</h2>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig7.png" class="" title="Alt text">
<blockquote>
<p>å›¾ 7ï¼šå·¦ï¼šViT-L/32 RGB å€¼çš„åˆå§‹çº¿æ€§åµŒå…¥çš„æ»¤æ³¢å™¨ã€‚ä¸­ï¼šViT-L/32 ä½ç½®åµŒå…¥çš„ç›¸ä¼¼æ€§ã€‚å›¾å—æ˜¾ç¤ºäº†å…·æœ‰æŒ‡å®šè¡Œå’Œåˆ—çš„è¡¥ä¸çš„ä½ç½®åµŒå…¥ä¸æ‰€æœ‰å…¶ä»–è¡¥ä¸çš„ä½ç½®åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§ã€‚å³å›¾ï¼šæŒ‰å¤´éƒ¨å’Œç½‘ç»œæ·±åº¦åˆ’åˆ†çš„å…³æ³¨åŒºåŸŸå¤§å°ã€‚æ¯ä¸ªç‚¹æ˜¾ç¤ºä¸€å±‚ 16 ä¸ªå¤´éƒ¨ä¹‹ä¸€çš„å›¾åƒä¹‹é—´çš„å¹³å‡æ³¨æ„åŠ›è·ç¦»ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è§é™„å½• D.7ã€‚</p>
</blockquote>
<p>1ï¸âƒ£ left: The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1). Figure 7 (left) shows the top principal components of the the learned embedding filters. The components resemble plausible basis functions for a low-dimensional representation of the fine structure within each patch.<br>
Vision Transformer çš„ç¬¬ä¸€å±‚å°†å±•å¹³çš„é¢ç‰‡çº¿æ€§æŠ•å½±åˆ°ä½ç»´ç©ºé—´ï¼ˆæ–¹ç¨‹ 1ï¼‰ã€‚å›¾ 7ï¼ˆå·¦ï¼‰æ˜¾ç¤ºäº†å­¦ä¹ åˆ°çš„åµŒå…¥è¿‡æ»¤å™¨çš„é¡¶éƒ¨ä¸»è¦ç»„ä»¶ã€‚è¿™äº›ç»„ä»¶ç±»ä¼¼äºæ¯ä¸ªè¡¥ä¸å†…ç²¾ç»†ç»“æ„çš„ä½ç»´è¡¨ç¤ºçš„åˆç†åŸºå‡½æ•°ã€‚</p>
<p>2ï¸âƒ£center: Figure 7 (center) shows that the model learns to encode distance within the image in the similarity of position embeddings, i.e. closer patches tend to have more similar position embeddings. Further, the row-column structure appears; patches in the same row/column have similar embeddings. Finally, a sinusoidal structure is sometimes apparent for larger grids (Appendix D). That the position embeddings learn to represent 2D image topology explains why hand-crafted 2D-aware embedding variants do not yield improvements (Appendix D.4)å›¾ 7ï¼ˆä¸­ï¼‰æ˜¾ç¤ºï¼Œæ¨¡å‹å­¦ä¹ ä»¥ä½ç½®åµŒå…¥çš„ç›¸ä¼¼æ€§å¯¹å›¾åƒå†…çš„è·ç¦»è¿›è¡Œç¼–ç ï¼Œå³è¾ƒè¿‘çš„å—å¾€å¾€å…·æœ‰æ›´ç›¸ä¼¼çš„ä½ç½®åµŒå…¥ã€‚è¿›ä¸€æ­¥ï¼Œå‡ºç°è¡Œåˆ—ç»“æ„ï¼›åŒä¸€è¡Œ/åˆ—ä¸­çš„è¡¥ä¸å…·æœ‰ç›¸ä¼¼çš„åµŒå…¥ã€‚æœ€åï¼Œå¯¹äºè¾ƒå¤§çš„ç½‘æ ¼ï¼Œæœ‰æ—¶ä¼šå‡ºç°æ˜æ˜¾çš„æ­£å¼¦ç»“æ„ï¼ˆé™„å½• Dï¼‰ã€‚ä½ç½®åµŒå…¥å­¦ä¹ è¡¨ç¤º 2D å›¾åƒæ‹“æ‰‘è§£é‡Šäº†ä¸ºä»€ä¹ˆæ‰‹å·¥åˆ¶ä½œçš„ 2D æ„ŸçŸ¥åµŒå…¥å˜ä½“æ— æ³•äº§ç”Ÿæ”¹è¿›ï¼ˆé™„å½• D.4ï¼‰</p>
<hr>
<p><em><strong>attention distance</strong></em></p>
<p>3ï¸âƒ£right: Self-attention allows ViT to integrate information across the entire image even in the lowest layers.è‡ªæ³¨æ„åŠ›ä½¿å¾— ViT èƒ½å¤Ÿæ•´åˆæ•´ä¸ªå›¾åƒçš„ä¿¡æ¯ï¼Œç”šè‡³åœ¨æœ€ä½å±‚ä¹Ÿæ˜¯å¦‚æ­¤ã€‚<br>
Specifically, we compute the average distance in image space across which information is integrated, based on the attention weights<br>
å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ ¹æ®æ³¨æ„åŠ›æƒé‡è®¡ç®—å›¾åƒç©ºé—´ä¸­ä¿¡æ¯æ•´åˆçš„å¹³å‡è·ç¦»</p>
<p>â­ğŸ˜®This â€œattention distanceâ€ is analogous to receptive field size in CNNs. We find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model.<strong>è¿™ç§â€œæ³¨æ„åŠ›è·ç¦»â€ç±»ä¼¼äº CNN ä¸­çš„æ„Ÿå—é‡å¤§å°</strong>ã€‚æˆ‘ä»¬å‘ç°ä¸€äº›å¤´éƒ¨å·²ç»å…³æ³¨äº†æœ€ä½å±‚ä¸­çš„å¤§éƒ¨åˆ†å›¾åƒï¼Œè¿™è¡¨æ˜è¯¥æ¨¡å‹ç¡®å®ä½¿ç”¨äº†å…¨å±€é›†æˆä¿¡æ¯çš„èƒ½åŠ›ã€‚</p>
<p>â­â­</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig11.png" class="" title="Alt text">
<p>å›¾ 11ï¼šæŒ‰å¤´éƒ¨å’Œç½‘ç»œæ·±åº¦åˆ’åˆ†çš„å…³æ³¨åŒºåŸŸå¤§å°ã€‚é€šè¿‡å¹³å‡æŸ¥è¯¢åƒç´ ä¸æ‰€æœ‰å…¶ä»–åƒç´ ä¹‹é—´çš„è·ç¦»ï¼ˆæŒ‰æ³¨æ„åŠ›æƒé‡åŠ æƒï¼‰ï¼Œè®¡ç®— 128 ä¸ªç¤ºä¾‹å›¾åƒçš„æ³¨æ„åŠ›è·ç¦»ã€‚æ¯ä¸ªç‚¹æ˜¾ç¤ºä¸€å±‚ 16 ä¸ªå¤´éƒ¨ä¹‹ä¸€çš„å›¾åƒä¹‹é—´çš„å¹³å‡æ³¨æ„åŠ›è·ç¦»ã€‚å›¾åƒå®½åº¦ä¸º 224 åƒç´ ã€‚</p>
<ul>
<li>ä»å›¾ä¸­èƒ½çœ‹å‡ºä»€ä¹ˆï¼Ÿ</li>
<li>Average attention distance is highly variable across heads in lower layers, with some heads attending to much of the image, while others attend to small regions at or near the query location. As depth increases, attention distance increases for all heads.<br>
è¾ƒä½å±‚ä¸­å„ä¸ªå¤´éƒ¨çš„å¹³å‡æ³¨æ„åŠ›è·ç¦»å˜åŒ–å¾ˆå¤§ï¼Œä¸€äº›å¤´éƒ¨å…³æ³¨å›¾åƒçš„å¤§éƒ¨åˆ†ï¼Œè€Œå…¶ä»–å¤´éƒ¨åˆ™å…³æ³¨æŸ¥è¯¢ä½ç½®å¤„æˆ–é™„è¿‘çš„å°åŒºåŸŸã€‚éšç€æ·±åº¦çš„å¢åŠ ï¼Œæ‰€æœ‰å¤´éƒ¨çš„æ³¨æ„åŠ›è·ç¦»éƒ½ä¼šå¢åŠ ã€‚</li>
</ul>
<hr>
<p>Globally, we find that the model attends to image regions that are semantically relevant for classification<br>
æˆ‘ä»¬å‘ç°è¯¥æ¨¡å‹å…³æ³¨ä¸åˆ†ç±»è¯­ä¹‰ç›¸å…³çš„å›¾åƒåŒºåŸŸ</p>
<h2 id="å€¼å¾—æ³¨æ„çš„ç»†èŠ‚ğŸ˜ğŸ˜ğŸ˜">å€¼å¾—æ³¨æ„çš„ç»†èŠ‚ğŸ˜ğŸ˜ğŸ˜</h2>
<ul>
<li>we do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step. Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP<br>
é™¤äº†åˆå§‹è¡¥ä¸æå–æ­¥éª¤ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¸ä¼šå°†ç‰¹å®šäºå›¾åƒçš„å½’çº³åå·®å¼•å…¥åˆ°æ¶æ„ä¸­ã€‚ç›¸åï¼Œæˆ‘ä»¬<code>å°†å›¾åƒè§£é‡Šä¸ºä¸€ç³»åˆ—è¡¥ä¸</code>ï¼Œå¹¶é€šè¿‡ NLP ä¸­ä½¿ç”¨çš„æ ‡å‡† Transformer ç¼–ç å™¨å¯¹å…¶è¿›è¡Œå¤„ç†</li>
</ul>
<hr>
<ul>
<li>For ImageNet we found it beneficial to additionally apply gradient clipping at global norm 1<br>
å¯¹äº ImageNetï¼Œæˆ‘ä»¬å‘ç°åœ¨å…¨å±€èŒƒæ•° 1 ä¸‹é¢å¤–åº”ç”¨æ¢¯åº¦è£å‰ªæ˜¯æœ‰ç›Šçš„ <a href="#%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86">è¡¥å……çŸ¥è¯†</a></li>
</ul>
<hr>
<ul>
<li>ä½œè€…å¯¹ç½‘ç»œç»“æ„è¿›è¡Œäº†æ¢ç´¢</li>
</ul>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig8.png" class="" title="Alt text">
<p>Decreasing the patch size and thus increasing the effective sequence length shows surprisingly robust improvements without introducing parameters.<br>
å‡å°è¡¥ä¸å¤§å°å¹¶ä»è€Œå¢åŠ æœ‰æ•ˆåºåˆ—é•¿åº¦åœ¨ä¸å¼•å…¥å‚æ•°çš„æƒ…å†µä¸‹æ˜¾ç¤ºå‡ºä»¤äººæƒŠè®¶çš„å¼ºå¤§æ”¹è¿›ã€‚</p>
<p>These findings suggest that compute might be a better predictor of performance than the number of parameters, and that scaling should emphasize depth over width if any.<br>
è¿™äº›å‘ç°è¡¨æ˜ï¼Œè®¡ç®—å¯èƒ½æ¯”å‚æ•°æ•°é‡æ›´å¥½åœ°é¢„æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”ç¼©æ”¾åº”è¯¥å¼ºè°ƒæ·±åº¦è€Œä¸æ˜¯å®½åº¦ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚</p>
<hr>
<h3 id="Axial-AttentionğŸ˜•">Axial AttentionğŸ˜•</h3>
<p>The general idea of axial attention is to perform multiple attention operations, each along a single axis of the input tensor, instead of applying 1-dimensional attention to the flattened version of the input.<br>
è½´å‘æ³¨æ„åŠ›çš„æ€»ä½“æ€æƒ³æ˜¯æ‰§è¡Œå¤šä¸ªæ³¨æ„åŠ›æ“ä½œï¼Œæ¯ä¸ªæ³¨æ„åŠ›æ“ä½œéƒ½æ²¿ç€è¾“å…¥å¼ é‡çš„å•ä¸ªè½´ï¼Œè€Œä¸æ˜¯å¯¹è¾“å…¥çš„æ‰å¹³ç‰ˆæœ¬åº”ç”¨ä¸€ç»´æ³¨æ„åŠ›ã€‚<br>
In axial attention, each attention mixes information along a particular axis, while keeping information along the other axes independent.<br>
åœ¨è½´å‘æ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªæ³¨æ„åŠ›æ··åˆæ²¿ç‰¹å®šè½´çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒæ²¿å…¶ä»–è½´çš„ä¿¡æ¯ç‹¬ç«‹ã€‚</p>
<ul>
<li>
<p>Axial Attention æ•ˆæœæ€ä¹ˆæ ·ï¼Ÿ</p>
<blockquote>
<p>æ€§èƒ½æ›´å¥½ï¼Œè®¡ç®—æ›´å¤§</p>
</blockquote>
</li>
<li>
<p>ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ</p>
<blockquote>
<p>This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column selfattention and although the sequence length that self-attention operates on is smaller in axial case, there is a extra MLP per Axial-ViT block.<br>
è¿™æ˜¯å› ä¸ºåœ¨ Axial-ViT æ¨¡å‹ä¸­ï¼Œæ¯ä¸ªå…·æœ‰å…¨å±€è‡ªæ³¨æ„åŠ›çš„ Transformer å—è¢«ä¸¤ä¸ª Axial Transformer å—å–ä»£ï¼Œä¸€ä¸ªå…·æœ‰è¡Œè‡ªæ³¨æ„åŠ›ï¼Œä¸€ä¸ªå…·æœ‰åˆ—è‡ªæ³¨æ„åŠ›ï¼Œå°½ç®¡è‡ªæ³¨æ„åŠ›åœ¨è½´å‘æƒ…å†µä¸‹è¿è¡Œçš„åºåˆ—é•¿åº¦è¾ƒå°ï¼Œæ¯ä¸ª Axial-ViT å—éƒ½æœ‰ä¸€ä¸ªé¢å¤–çš„ MLP</p>
</blockquote>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/fig13.png" class="" title="Alt text">
</li>
</ul>
<h3 id="fine-tuning-å¾®è°ƒ">fine-tuning å¾®è°ƒ</h3>
<p>â€œrunning fine-tuning at different resolution than training is common practiceâ€ ([Dosovitskiy ç­‰, 2021, p. 14](Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.)) ğŸ”¤ä»¥ä¸è®­ç»ƒä¸åŒçš„åˆ†è¾¨ç‡è¿›è¡Œå¾®è°ƒæ˜¯å¸¸è§çš„åšæ³•ğŸ”¤</p>
<p>ViT fine-tuning resolution: 384</p>
<ul>
<li>ğŸ˜•å¦‚ä½•fine-tuning
<ul>
<li>pre-train on large datasets;</li>
<li>remove the pre-trained prediction head and attach a zero-initialized D Ã— K feedforward layer, where K is the number of downstream classes.åˆ é™¤é¢„è®­ç»ƒçš„é¢„æµ‹å¤´å¹¶é™„åŠ ä¸€ä¸ªé›¶åˆå§‹åŒ–çš„ D Ã— K å‰é¦ˆå±‚ï¼Œå…¶ä¸­ K æ˜¯ä¸‹æ¸¸ç±»çš„æ•°é‡ã€‚</li>
<li>fine-tune at higher resolution, keep the patch size the same</li>
<li>perform 2D interpolation of the pre-trained position embeddings</li>
</ul>
</li>
</ul>
<h3 id="self-supervision">self-supervision</h3>
<p>Self-Supervised Learningï¼Œåˆç§°ä¸ºè‡ªç›‘ç£å­¦ä¹ ï¼Œæœºå™¨å­¦ä¹ ä¸€èˆ¬åˆ†ä¸ºæœ‰ç›‘ç£å­¦ä¹ ï¼Œæ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ Self-Supervised Learning æ˜¯æ— ç›‘ç£å­¦ä¹ é‡Œé¢çš„ä¸€ç§ï¼Œå¸Œæœ›å­¦ä¹ ä¸€ç§é€šç”¨çš„ç‰¹å¾è¡¨è¾¾ç”¨äºä¸‹æ¸¸ä»»åŠ¡ (Downstream Tasks)ã€‚ ä¸»è¦é€šè¿‡è‡ªå·±ç›‘ç£è‡ªå·±ã€‚æ¯”å¦‚æŠŠä¸€æ®µè¯é‡Œé¢çš„å‡ ä¸ªå•è¯å»æ‰ï¼Œç”¨ä»–çš„ä¸Šä¸‹æ–‡å»é¢„æµ‹ç¼ºå¤±çš„å•è¯ï¼Œæˆ–è€…å°†å›¾ç‰‡çš„ä¸€äº›éƒ¨åˆ†å»æ‰ï¼Œä¾èµ–å…¶å‘¨å›´çš„ä¿¡æ¯å»é¢„æµ‹ç¼ºå¤±çš„ patchã€‚</p>
<ul>
<li>
<p>ä¸ºä»€ä¹ˆé‡‡ç”¨self-supervision?</p>
<blockquote>
<p>Transformers show impressive performance on NLP tasks. However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training<br>
Transformer åœ¨ NLP ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä»–ä»¬çš„æˆåŠŸå¾ˆå¤§ç¨‹åº¦ä¸Šä¸ä»…æºäºå…¶å‡ºè‰²çš„å¯æ‰©å±•æ€§ï¼Œè¿˜æºäºå¤§è§„æ¨¡çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒ</p>
</blockquote>
</li>
<li>
<p>å“ªç§æ–¹å¼ï¼Ÿ</p>
<blockquote>
<p>masked patch prediction</p>
</blockquote>
<blockquote>
<p>To do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable [mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is (10%).<br>
ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶åµŒå…¥æ›¿æ¢ä¸ºå¯å­¦ä¹ çš„[æ©ç ]åµŒå…¥ï¼ˆ80ï¼…ï¼‰ã€éšæœºçš„å…¶ä»–è¡¥ä¸åµŒå…¥ï¼ˆ10ï¼…ï¼‰æˆ–ä»…ä¿æŒåŸæ ·ï¼ˆ10ï¼…ï¼‰æ¥ç ´å50ï¼…çš„è¡¥ä¸åµŒå…¥ã€‚</p>
</blockquote>
</li>
<li>
<p>ç»“è®º<br>
Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pretraining.<br>
æˆ‘ä»¬çš„åˆæ­¥å®éªŒæ˜¾ç¤ºè‡ªç›‘ç£é¢„è®­ç»ƒæœ‰æ‰€æ”¹å–„ï¼Œä½†è‡ªç›‘ç£é¢„è®­ç»ƒä¸å¤§è§„æ¨¡ç›‘ç£é¢„è®­ç»ƒä¹‹é—´ä»ç„¶å­˜åœ¨å¾ˆå¤§å·®è·ã€‚</p>
</li>
</ul>
<h2 id="è¡¥å……çŸ¥è¯†">è¡¥å……çŸ¥è¯†</h2>
<h3 id="æ¢¯åº¦æˆªæ–­">æ¢¯åº¦æˆªæ–­</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/csnc007/article/details/97804398">æ¢¯åº¦çˆ†ç‚¸è§£å†³æ–¹æ¡ˆâ€”â€”æ¢¯åº¦æˆªæ–­ï¼ˆgradient clip normï¼‰</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/203085892">æ·±åº¦å­¦ä¹ ä¹‹æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/379032332">Gradient Clippingç†è§£</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353871841">æ¢¯åº¦è£å‰ªclip_grad_normå’Œclip_gradient</a></p>
<p>æ¢¯åº¦å‰ªè£æ˜¯ä¸€ç§åœ¨ç½‘ç»œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå°†è¯¯å·®å¯¼æ•°æ”¹å˜æˆ–å‰ªè£åˆ°é˜ˆå€¼ï¼Œå¹¶åˆ©ç”¨å‰ªè£åçš„æ¢¯åº¦æ¥æ›´æ–°æƒå€¼çš„æ–¹æ³•ã€‚</p>
<p>æˆ‘ä»¬å¯ä»¥é‡‡å–ä¸€ä¸ªç®€å•çš„ç­–ç•¥æ¥é¿å…æ¢¯åº¦çš„çˆ†ç‚¸ï¼Œé‚£å°±æ˜¯æ¢¯åº¦æˆªæ–­Clip, å°†æ¢¯åº¦çº¦æŸåœ¨æŸä¸€ä¸ªåŒºé—´ä¹‹å†…ï¼Œåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œåœ¨ä¼˜åŒ–å™¨æ›´æ–°ä¹‹å‰è¿›è¡Œæ¢¯åº¦æˆªæ–­æ“ä½œã€‚</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/gradient_clipping.png" class="" title="æ¢¯åº¦è£å‰ª">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn.utils <span class="keyword">import</span> clip_grad_norm</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()        </span><br><span class="line">loss, hidden = model(data, hidden, targets)</span><br><span class="line">loss.backward()</span><br><span class="line"> </span><br><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)</span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¿™ä¸ªå‡½æ•°è®¡ç®—çš„æ˜¯å…¨å±€æ¢¯åº¦èŒƒæ•°</span></span><br><span class="line"><span class="comment"># torch.nn.utils.clip_grad_norm(parameters=model.parameters(), max_norm=5, norm_type=2)</span></span><br><span class="line"><span class="comment"># parameters: an iterable of Variables that will have gradients normalized</span></span><br><span class="line"><span class="comment"># max_norm: max norm of the gradients(é˜ˆå€¼è®¾å®š)</span></span><br><span class="line"><span class="comment"># norm_type: type of the used p-norm. Can be&#x27;inf&#x27;for infinity norm(å®šä¹‰èŒƒæ•°ç±»å‹)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353871841">https://zhuanlan.zhihu.com/p/353871841</a></p>
<h3 id="few-shot-learning-å°‘æ ·æœ¬å­¦ä¹ ">few-shot learning(å°‘æ ·æœ¬å­¦ä¹ )</h3>
<p>Few-shot Learning æ˜¯ Meta Learning åœ¨ç›‘ç£å­¦ä¹ é¢†åŸŸçš„åº”ç”¨ã€‚<br>
Meta Learningï¼Œåˆç§°ä¸ºlearning to learnï¼Œè¯¥ç®—æ³•æ—¨åœ¨è®©æ¨¡å‹å­¦ä¼šâ€œå­¦ä¹ â€ï¼Œèƒ½å¤Ÿå¤„ç†ç±»å‹ç›¸ä¼¼çš„ä»»åŠ¡ï¼Œè€Œä¸æ˜¯åªä¼šå•ä¸€çš„åˆ†ç±»ä»»åŠ¡ã€‚</p>
<p>few-shot learning ä¸­æœ‰ä¸€ä¸ªæœ¯è¯­å«åš N-way K -shot é—®é¢˜ã€‚<br>
å½¢å¼åŒ–æ¥è¯´ï¼Œfew-shot çš„è®­ç»ƒé›†ä¸­åŒ…å«äº†å¾ˆå¤šçš„ç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«ä¸­æœ‰å¤šä¸ªæ ·æœ¬ã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œä¼šåœ¨è®­ç»ƒé›†ä¸­éšæœºæŠ½å– N ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ« K ä¸ªæ ·æœ¬ï¼ˆæ€»å…± N<em>K ä¸ªæ•°æ®ï¼‰ï¼Œæ„å»ºä¸€ä¸ª meta-taskï¼Œä½œä¸ºæ¨¡å‹çš„æ”¯æ’‘é›†ï¼ˆsupport setï¼‰è¾“å…¥ï¼›å†ä»è¿™ N ä¸ªç±»ä¸­å‰©ä½™çš„æ•°æ®ä¸­æŠ½å–ä¸€æ‰¹ï¼ˆbatchï¼‰æ ·æœ¬ä½œä¸ºæ¨¡å‹çš„é¢„æµ‹å¯¹è±¡ï¼ˆbatch setï¼‰ã€‚å³è¦æ±‚æ¨¡å‹ä» N</em>K ä¸ªæ•°æ®ä¸­å­¦ä¼šå¦‚ä½•åŒºåˆ†è¿™ N ä¸ªç±»åˆ«ï¼Œè¿™æ ·çš„ä»»åŠ¡è¢«ç§°ä¸º N-way K-shot é—®é¢˜ã€‚</p>
<p>å›¾2å±•ç¤ºçš„æ˜¯ä¸€ä¸ª 2-way 5-shot çš„ç¤ºä¾‹ï¼Œå¯ä»¥çœ‹åˆ° meta training é˜¶æ®µæ„å»ºäº†ä¸€ç³»åˆ— meta-task æ¥è®©æ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ® support set é¢„æµ‹ batch set ä¸­çš„æ ·æœ¬çš„æ ‡ç­¾ï¼›meta testing é˜¶æ®µçš„è¾“å…¥æ•°æ®çš„å½¢å¼ä¸è®­ç»ƒé˜¶æ®µä¸€è‡´ï¼ˆ2-way 5-shotï¼‰ï¼Œä½†æ˜¯ä¼šåœ¨å…¨æ–°çš„ç±»åˆ«ä¸Šæ„å»º support set å’Œ batchã€‚</p>
<img src="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/few-shot.png" class="" title="Alt text">
<h2 id="å¥½è¯ä½³å¥ç§¯ç´¯">å¥½è¯ä½³å¥ç§¯ç´¯</h2>
<ul>
<li>
<p>While large ViT models <strong>perform worse</strong> than BiT ResNets (shaded area) when pre-trained on small datasets, they <strong>shine</strong> when pre-trained on larger datasets. Similarly, larger ViT variants <strong>overtake</strong> smaller ones as the dataset grows.</p>
</li>
<li>
<p>ResNets perform better with smaller pre-training datasets <strong>but plateau sooner than</strong> ViT, which performs better with larger pre-training.</p>
</li>
</ul>
<h2 id="é™„å½•">é™„å½•</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py#L83">GitHub 15kâ­ä»£ç </a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br><span class="line"><span class="comment"># helpers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pair</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">else</span> (t, t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># classes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads = <span class="number">8</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head *  heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line"></span><br><span class="line">        self.attend = nn.Softmax(dim = -<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.to_out = nn.Sequential(</span><br><span class="line">            nn.Linear(inner_dim, dim),</span><br><span class="line">            nn.Dropout(dropout)</span><br><span class="line">        ) <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.norm(x)</span><br><span class="line"></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim = -<span class="number">1</span>)</span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(t, <span class="string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = self.heads), qkv)</span><br><span class="line"></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale</span><br><span class="line"></span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        out = torch.matmul(attn, v)</span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.layers = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            self.layers.append(nn.ModuleList([</span><br><span class="line">                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),</span><br><span class="line">                FeedForward(dim, mlp_dim, dropout = dropout)</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = <span class="string">&#x27;cls&#x27;</span>, channels = <span class="number">3</span>, dim_head = <span class="number">64</span>, dropout = <span class="number">0.</span>, emb_dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        image_height, image_width = pair(image_size)</span><br><span class="line">        patch_height, patch_width = pair(patch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width == <span class="number">0</span>, <span class="string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width // patch_width)</span><br><span class="line">        patch_dim = channels * patch_height * patch_width</span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;<span class="string">&#x27;cls&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>&#125;, <span class="string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span></span><br><span class="line"></span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(<span class="string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_height, p2 = patch_width),</span><br><span class="line">            nn.LayerNorm(patch_dim),</span><br><span class="line">            nn.Linear(patch_dim, dim),</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)</span><br><span class="line"></span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.mlp_head = nn.Linear(dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        x = self.to_patch_embedding(img)</span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&#x27;1 1 d -&gt; b 1 d&#x27;</span>, b = b)</span><br><span class="line">        x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line"></span><br><span class="line">        x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">John Doe</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/">http://example.com/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Classical-Paper/">Classical Paper</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/26/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2023_Video_Frame_Interpolation_Survey/" title="ã€Videoã€‘Video Frame Interpolation A Comprehensive Survey"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">ã€Videoã€‘Video Frame Interpolation A Comprehensive Survey</div></div></a></div><div class="next-post pull-right"><a href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-12</div><div class="title">ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">John Doe</div><div class="author-info__description">Galaxy</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Tittle: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">1.1.</span> <span class="toc-text">Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E4%BD%93%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-number">1.1.1.</span> <span class="toc-text">æ€»ä½“æµç¨‹ï¼š</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Patch-embeddings"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Patch embeddings</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#class-token"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">class token</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Position-Embeddings"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">Position Embeddings</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%A9%E4%B8%8B%E7%9A%84%E5%B0%B1%E6%98%AFBN-MLP%E4%B9%8B%E7%B1%BB%E7%9A%84%E6%A8%A1%E5%9D%97%EF%BC%8C%E4%B8%8D%E9%9A%BE"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">å‰©ä¸‹çš„å°±æ˜¯BN,MLPä¹‹ç±»çš„æ¨¡å—ï¼Œä¸éš¾</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiments"><span class="toc-number">1.2.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3ViT%F0%9F%98%B3%F0%9F%98%B3"><span class="toc-number">1.3.</span> <span class="toc-text">å¦‚ä½•ç†è§£ViTğŸ˜³ğŸ˜³</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%80%BC%E5%BE%97%E6%B3%A8%E6%84%8F%E7%9A%84%E7%BB%86%E8%8A%82%F0%9F%98%8F%F0%9F%98%8F%F0%9F%98%8F"><span class="toc-number">1.4.</span> <span class="toc-text">å€¼å¾—æ³¨æ„çš„ç»†èŠ‚ğŸ˜ğŸ˜ğŸ˜</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Axial-Attention%F0%9F%98%95"><span class="toc-number">1.4.1.</span> <span class="toc-text">Axial AttentionğŸ˜•</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fine-tuning-%E5%BE%AE%E8%B0%83"><span class="toc-number">1.4.2.</span> <span class="toc-text">fine-tuning å¾®è°ƒ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-supervision"><span class="toc-number">1.4.3.</span> <span class="toc-text">self-supervision</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86"><span class="toc-number">1.5.</span> <span class="toc-text">è¡¥å……çŸ¥è¯†</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%88%AA%E6%96%AD"><span class="toc-number">1.5.1.</span> <span class="toc-text">æ¢¯åº¦æˆªæ–­</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#few-shot-learning-%E5%B0%91%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.5.2.</span> <span class="toc-text">few-shot learning(å°‘æ ·æœ¬å­¦ä¹ )</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A5%BD%E8%AF%8D%E4%BD%B3%E5%8F%A5%E7%A7%AF%E7%B4%AF"><span class="toc-number">1.6.</span> <span class="toc-text">å¥½è¯ä½³å¥ç§¯ç´¯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">1.7.</span> <span class="toc-text">é™„å½•</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/10/27/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2018_SuperSloMo/" title="ã€Videoã€‘Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ã€Videoã€‘Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation"/></a><div class="content"><a class="title" href="/2023/10/27/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2018_SuperSloMo/" title="ã€Videoã€‘Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation">ã€Videoã€‘Super SloMo High Quality Estimation of Multiple Intermediate Frames for Video Interpolation</a><time datetime="2023-10-27T03:16:59.000Z" title="Created 2023-10-27 11:16:59">2023-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/26/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2023_Video_Frame_Interpolation_Survey/" title="ã€Videoã€‘Video Frame Interpolation A Comprehensive Survey"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ã€Videoã€‘Video Frame Interpolation A Comprehensive Survey"/></a><div class="content"><a class="title" href="/2023/10/26/%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3/2023_Video_Frame_Interpolation_Survey/" title="ã€Videoã€‘Video Frame Interpolation A Comprehensive Survey">ã€Videoã€‘Video Frame Interpolation A Comprehensive Survey</a><time datetime="2023-10-26T03:28:12.000Z" title="Created 2023-10-26 11:28:12">2023-10-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/" title="ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"/></a><div class="content"><a class="title" href="/2023/10/23/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2021_ViT/" title="ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE">ã€Classical Paperã€‘AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a><time datetime="2023-10-23T07:23:33.000Z" title="Created 2023-10-23 15:23:33">2023-10-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks"/></a><div class="content"><a class="title" href="/2023/10/12/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/2012_AlexNet/" title="ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks">ã€Classical Paperã€‘ImageNet Classification with Deep Convolutional Neural Networks</a><time datetime="2023-10-12T03:29:47.000Z" title="Created 2023-10-12 11:29:47">2023-10-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/10/First-post/" title="First post"><img src="/img/cat.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="First post"/></a><div class="content"><a class="title" href="/2023/10/10/First-post/" title="First post">First post</a><time datetime="2023-10-10T05:11:00.000Z" title="Created 2023-10-10 13:11:00">2023-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By John Doe</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":320,"height":480},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body></html>